## todo
- [x] Tag spelling mistakes words as 0
- [x] Conv1DWithMasking
- [x] mask zero in all layers
- [x] Aggregation layer
- [x] QWK metric for evaluation
- [x] Get all glove data to decrease spelling mistakes
- [ ] Make Trainable True in embeddings layer
- [ ] Optimizer clipping
- [ ] Tag named entities @ to a new encoding
- [ ] Tune params and train with all data

## questions

- [x] What does a convolutional layer do in NLP context? Specifically how does it convert a 2000 x 300 -> 2000 x 1 with one filter
- [x] Choice of hyperparatmeters loss optimizer metric (QWK?) activations (sigmoid or softmax)
- [x] Which RNN to use?
- [x] Is this classification or regression?
- [x] Overall score - Induvidual score relation (deterministic or learnable)
- [x] How to proceed with more data once model is complete?
- [x] Strange behaviour Softmax - no change in loss, sigmoid - decreasing loss?
- [x] sigmoid/softmax activations?
